

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Entropy, Information and Bias Correction Primer &mdash; pyEntropy v0.5.0 documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.5.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="pyEntropy v0.5.0 documentation" href="index.html" />
    <link rel="next" title="Module Reference" href="api.html" />
    <link rel="prev" title="Examples (Getting Started Guide)" href="examples.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="np-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api.html" title="Module Reference"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="examples.html" title="Examples (Getting Started Guide)"
             accesskey="P">previous</a> |</li>
  <li><a href="http://code.google.com/p/pyentropy">Project Home</a> |&nbsp;</li>
  <li><a href="index.html">Documentation</a> &raquo;</li>
 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Entropy, Information and Bias Correction Primer</a><ul>
<li><a class="reference internal" href="#entropy-and-mutual-information">Entropy and Mutual Information</a></li>
<li><a class="reference internal" href="#bias">Bias</a><ul>
<li><a class="reference internal" href="#origins-of-sampling-bias">Origins of sampling bias</a></li>
<li><a class="reference internal" href="#bias-correction-methods">Bias correction methods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pyentropy">pyEntropy</a><ul>
<li><a class="reference internal" href="#bias-corrections">Bias Corrections</a><ul>
<li><a class="reference internal" href="#which-method-should-i-use">Which method should I use?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#entropy-values">Entropy Values</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="examples.html"
                        title="previous chapter">Examples (Getting Started Guide)</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="api.html"
                        title="next chapter">Module Reference</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="entropy-information-and-bias-correction-primer">
<span id="primer"></span><h1>Entropy, Information and Bias Correction Primer<a class="headerlink" href="#entropy-information-and-bias-correction-primer" title="Permalink to this headline">¶</a></h1>
<div class="section" id="entropy-and-mutual-information">
<h2>Entropy and Mutual Information<a class="headerlink" href="#entropy-and-mutual-information" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy</a> is a
measure of uncertainty.  This is a function of a discrete probability
distribution and is defined as the expected value of the surprise.  The
surprise of a particular observation is equal to log of the reciprocal of the
probability of the observation &#8212; for an observation <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> the surprise is
<img class="math" src="_images/math/749be2675b63f7336a6b55d21afb149dd1b11f04.png" alt="-\log p(x)"/>.  An event with a large probability will have a small
surprise (since it is relatively likely to occur); an event with a small
probability will have a high surprise (since it is less likely to occur).  The
entropy is then the average of this value over all possible observations,
weighted by the probability of each observation.  For a finite discrete set,
the distribution with maximum entropy is the uniform distribution, where each
possible observation has an equal chance of occurrence, for example rolling a
fair die.  Since in this case nothing can be predicted about the outcome of any
particular observation, it is the most uncertain distribution possible.
Similarly, the distribution with minimum entropy would be one where the
probability of one specific event is equal to 1 and the probability of all
other events is 0.  In this case, there is no uncertainty about any particular
observation and the entropy is zero. In general, a more evenly distributed
probability density will result in a distribution with higher entropy.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Mutual_information">Mutual information</a> (or just
information) is a function of two random variables (for example the input and
output of a system).  It is defined as the average reduction in uncertainty
(measured by entropy) about one of the random variables after the other is
observed.  If the two variables are completely independent, then observing the
outcome of one will have no effect on the uncertainty of the other, and hence
the information between them is 0.  An alternative but equivalent definition is
that information is the Kullbeck-Leibler divergence (a measure of distance
between two probability distributions) between the joint distribution of the
two variables, and the product of the individual distribution of each variables
(the independent distribution).  It therefore quantifies how far the joint
distribution is from independence, or how strongly the two variables are
correlated.  When the logarithm used in the definition of the surprise is
base-2, as is conventionally the case, the units of the information are called
<em>bits</em>.  1 bit corresponds to a reduction of uncertainty of a factor of 2.
For example, if a die is rolled and an observer is told that the resulting
number is odd, this reduces the number of possible outcomes for this
observation from 6 to 3 &#8212; this is 1 bit of information.</p>
</div>
<div class="section" id="bias">
<h2>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h2>
<div class="section" id="origins-of-sampling-bias">
<h3>Origins of sampling bias<a class="headerlink" href="#origins-of-sampling-bias" title="Permalink to this headline">¶</a></h3>
<p>Calculating entropy and information requires estimates of the underlying
probability distributions which must be obtained from experimental
observations. With an infinite amount of data, these distributions can be
learned exactly, but when limited data is available, the probability estimates
can be effected. For example, low probability events may not occur in the
experimental sample, which results in the variability of the distribution being
under estimated (the entropy is biased downwards).</p>
<p>The bias of the entropy is a big problem since information is calculated as the
difference of two entropies. This difference is often much smaller than the two
entropies involved, so even a very small proportional bias error in the entropy
can have a large effect on the information. Information is biased upwards,
since of the two constituent entropy quantities, the second (negative term) has
a larger downward bias. Intuitively, this can be because with a limited amount
of data, spurious chance corrleations between the variables can artificially
inflate the true information value.</p>
</div>
<div class="section" id="bias-correction-methods">
<h3>Bias correction methods<a class="headerlink" href="#bias-correction-methods" title="Permalink to this headline">¶</a></h3>
<p>There are a number of bias correction techniques that have been developed to
address this problem.</p>
<p>For more information see</p>
<blockquote>
<div><p>RAA Ince, RS Petersen, DC Swan and S Panzeri (2009) &#8220;Python for information
theoretic analysis of neural data&#8221; <em>Frontiers in Neuroinformatics</em> <strong>3</strong> 4
<a class="reference external" href="http://dx.doi.org/10.3389/neuro.11.004.2009">link</a></p>
<p>S Panzeri, R Senatore, MA Montemurro and RS Petersen (2007) &#8220;Correcting for
the sampling bias problem in spike train information measures&#8221; <em>Journal of
Neurophysiology</em> <strong>98</strong>:3 1064-1072</p>
</div></blockquote>
</div>
</div>
<div class="section" id="pyentropy">
<h2>pyEntropy<a class="headerlink" href="#pyentropy" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bias-corrections">
<h3>Bias Corrections<a class="headerlink" href="#bias-corrections" title="Permalink to this headline">¶</a></h3>
<p>pyEntropy currently implements the following bias correction methods, specified
with <cite>method</cite> argument to
<a class="reference internal" href="api.html#pyentropy.systems.BaseSystem.calculate_entropies" title="pyentropy.systems.BaseSystem.calculate_entropies"><tt class="xref py py-func docutils literal"><span class="pre">pyentropy.systems.BaseSystem.calculate_entropies()</span></tt></a>:</p>
<dl class="docutils">
<dt><tt class="docutils literal"><span class="pre">plugin</span></tt></dt>
<dd>The standard <em>plugin</em> or <em>naive</em> entropy estimator.</dd>
<dt><tt class="docutils literal"><span class="pre">pt</span></tt></dt>
<dd>Panzeri-Treves bias correction <a class="reference internal" href="#pt96">[PT96]</a>. This is the Miller-Madow analytic
correction, where the number of non-zero responses is estimated
using a Bayesian procedure rather than the naive count, yielding much
better results.</dd>
<dt><tt class="docutils literal"><span class="pre">qe</span></tt></dt>
<dd>Quadratic extrapolation <a class="reference internal" href="#strong98">[Strong98]</a>. See above.</dd>
<dt><tt class="docutils literal"><span class="pre">nsb</span></tt></dt>
<dd>Nemenman-Shafee-Bialek method <a class="reference internal" href="#nsb02">[NSB02]</a>.</dd>
</dl>
<div class="section" id="which-method-should-i-use">
<h4>Which method should I use?<a class="headerlink" href="#which-method-should-i-use" title="Permalink to this headline">¶</a></h4>
<p>This is a difficult question and is to some extent subjective, being a trade
off between bias, variance and computational requirements. It is complicated by
the fact that these factors are also affected by the statistics of the system
being studied. Generally is you have a single variable X space the best results
are obtained with the NSB estimator, but it is much slower to compute than the
other methods. If you have a multi-variable X space, the best results are
achieved with the shuffled estimator, corrected with either PT or QE. The best
thing to do, if possible, is to test the methods with simulated data with
similar statistics. Future versions of pyEntropy will include helper functions
to make this easier.</p>
</div>
</div>
<div class="section" id="entropy-values">
<h3>Entropy Values<a class="headerlink" href="#entropy-values" title="Permalink to this headline">¶</a></h3>
<p>pyEntropy currently implements the following entropy values, specified in
<cite>calc</cite> argument to <a class="reference internal" href="api.html#pyentropy.systems.BaseSystem.calculate_entropies" title="pyentropy.systems.BaseSystem.calculate_entropies"><tt class="xref py py-func docutils literal"><span class="pre">pyentropy.systems.BaseSystem.calculate_entropies()</span></tt></a>:</p>
<dl class="docutils">
<dt><tt class="docutils literal"><span class="pre">HX</span></tt></dt>
<dd><img class="math" src="_images/math/50f924b9548e41c8f9471078fadd897e6a839dfd.png" alt="H(\mathbf{X})"/> &#8211; unconditional entropy of X.</dd>
<dt><tt class="docutils literal"><span class="pre">HY</span></tt></dt>
<dd><img class="math" src="_images/math/24fe802a4d6be6f2e8adcf42cf6dd91308507052.png" alt="H(Y)"/> &#8211; unconditional entropy of Y.</dd>
<dt><tt class="docutils literal"><span class="pre">HshX</span></tt></dt>
<dd><img class="math" src="_images/math/c7393bb265a3971288ba7a528e38c0ae7c4eaad5.png" alt="H_{sh}(\mathbf{X})"/> &#8211; shuffle-decorrelated independent unconditional    entropy of X. (X components are shuffled).</dd>
<dt><tt class="docutils literal"><span class="pre">SiHXi</span></tt></dt>
<dd><img class="math" src="_images/math/972d32efeba207f527ca8eb1dfcdaf507748ad35.png" alt="\sum_{i=1}^{Xm} H(X_{i})"/> &#8211; direct-decorrelated independent
unconditional entropy X. Summing the individual entropies corresponds
to constructing the analytic independent joint distribution (product
of individual distributions).</dd>
<dt><tt class="docutils literal"><span class="pre">HXY</span></tt></dt>
<dd><img class="math" src="_images/math/27726c4d09a186a75c42059f5d5a1bef2c60f2c8.png" alt="H(\mathbf{X}|Y)"/> &#8211; entropy of X conditional on Y.</dd>
<dt><tt class="docutils literal"><span class="pre">HiXY</span></tt></dt>
<dd><img class="math" src="_images/math/af056adb9f2224b3a4b0944c144ceb03e53301d0.png" alt="H_{ind}(\mathbf{X}|Y) = \sum_{i=1}^{Xm} H(X_{i}|Y)"/> &#8211;
direct-decorrelated independent conditional entropy.</dd>
<dt><tt class="docutils literal"><span class="pre">HshXY</span></tt></dt>
<dd><img class="math" src="_images/math/a84c565e3304c0c5b790be633749167a9c3bc7df.png" alt="H_{sh}(\mathbf{X}|Y)"/> &#8211; shuffle-decorrelated independent
conditional entropy. X components are shuffled for each response Y (but not
between responses).</dd>
<dt><tt class="docutils literal"><span class="pre">HiX</span></tt></dt>
<dd><img class="math" src="_images/math/edc4ea76b65315c6640b57048ad410dfec50396c.png" alt="H_{ind}(\mathbf{X})"/> &#8211; unconditional direct-conditionally-decorrelated entropy of X. Entropy of <img class="math" src="_images/math/776a63867a995a0cac4232a8de5f48ccd9955f90.png" alt="P_{ind}(X) = \sum_{y \in Y}
P(y) \prod_{i=1}^{Xm} P(X_{i}|y)"/></dd>
<dt><tt class="docutils literal"><span class="pre">ChiXY</span></tt></dt>
<dd><img class="math" src="_images/math/b4b02bc50caf79eff760b7caf01f7a4d3c79194a.png" alt="\chi (\mathbf{X})"/> &#8211; cross entropy between <img class="math" src="_images/math/59f0a208bf0ecf13d4236e1f70e7be4f26fafc32.png" alt="P_{ind}(X)"/> and
<img class="math" src="_images/math/7eb3955b3897cf58a1709a54fc2496e0773f132f.png" alt="P(X)"/></dd>
</dl>
<table class="docutils citation" frame="void" id="pt96" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[PT96]</a></td><td>Panzeri, S and Treves, A (1996) &#8220;Analytical estimates of limited
sampling biases in different infromation measures&#8221; <em>Network</em> <strong>7</strong>:1 87-107</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="strong98" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[Strong98]</a></td><td>SP Strong, R Koberle, RR de Ruyter van Steveninck and W Bialek
(1998) &#8220;Entropy and information in neural spike trains&#8221; <em>Physical review letters</em>
<strong>80</strong>:1 197-200</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="nsb02" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[NSB02]</a></td><td>I Nemenman, F Shafee and W Bialek (2002) &#8220;Entropy and inference,
revisted&#8221; <em>NIPS</em> <strong>14</strong> 95-100</td></tr>
</tbody>
</table>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="np-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api.html" title="Module Reference"
             >next</a> |</li>
        <li class="right" >
          <a href="examples.html" title="Examples (Getting Started Guide)"
             >previous</a> |</li>
  <li><a href="http://code.google.com/p/pyentropy">Project Home</a> |&nbsp;</li>
  <li><a href="index.html">Documentation</a> &raquo;</li>
 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2009, Robin Ince.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7.
    </div>
  </body>
</html>